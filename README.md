# Smart Cities Project

## Workflow:

- Python files
  - [x] `air_structure.py`: Pull the data from the pollution link site 
  - [x] `traffic_structure.py`: Read from Barcelona transit site where to pull all the transit data in a .json file
- MongoDB
  - [x] `docker.compose.yml`: Docker file containing Grafana and MongoDB
  - [ ] Connect and upload all the data generated by the files
    - [x] `air_pollution.py` is the main puller their webpage
    - [ ] `traffic.py` is the main puller for their webpage
  - [x] Give a specific format
- Grafana
  - [x] Docker file
  - [ ] Connect and read values
  - [ ] Make/Get a dashboard
  - [ ] Show data


## Webapges to inspect:
- [Pollution link site](https://aqicn.org/map/barcelona/)
- [Barcelona transit data](https://com-shi-va.barcelona.cat/ca/transit)


## MongoDB dataset structure
The MongoDB collection will have two main documents, the `air_pollution` and the `traffic` one. Each one will save a document for every sensor we have avaliable via our API's, actualizing them each day/hour (to be determined) pushing the new information to it.

### Forks and ideas
A fork for making more redundant our idea would be to, instead using the `traffic` API, to use social media like Twitter to detect traffic jams and calculate the indirects effects that may have in the enviorment, making it a worse zone to pass by.